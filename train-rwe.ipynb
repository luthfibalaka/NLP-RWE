{"cells":[{"cell_type":"markdown","metadata":{"id":"d4CAaa8y2wrX"},"source":["# **ACL-19 Paper Experiment: Relational Word Embedding**\n","\n","[Paper](https://aclanthology.org/P19-1318.pdf) -\n","[Repository](https://github.com/pedrada88/rwe/)"]},{"cell_type":"markdown","metadata":{"id":"VAFTJRdp4LFo"},"source":["## **A. Import Packages & Data**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"FaMasgVm1XsC"},"outputs":[],"source":["# Import required packages\n","import sys\n","import random\n","import torch\n","import numpy as np\n","import os\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""]},{"cell_type":"markdown","metadata":{"id":"BbgMA_FE4U92"},"source":["## **B. Define Networks**"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"H_-uWQrX3tXV"},"outputs":[],"source":["# Define neural network model\n","class RWE_Model(torch.nn.Module):\n","    def __init__(\n","        self,\n","        embedding_size_input,\n","        embedding_size_output,\n","        embedding_weights,\n","        hidden_size,\n","        dropout,\n","    ):\n","        super(RWE_Model, self).__init__()\n","        self.embeddings = torch.nn.Embedding.from_pretrained(embedding_weights).float()\n","        self.embeddings.weight.requires_grad = True\n","        self.linear1 = torch.nn.Linear(embedding_size_input * 2, hidden_size)\n","        self.relu = torch.nn.ReLU()\n","        self.dropout = torch.nn.Dropout(dropout)\n","        self.linear2 = torch.nn.Linear(hidden_size, embedding_size_output)\n","\n","    def forward(self, input1, input2):\n","        embed1 = self.embeddings(input1)\n","        embed2 = self.embeddings(input2)\n","        out = self.linear1(\n","            torch.cat(((embed1 * embed2), (embed1 + embed2) / 2), 2)\n","        ).squeeze()\n","        out = self.relu(out)\n","        out = self.dropout(out)\n","        out = self.linear2(out)\n","        return out\n","\n","\n","# Define function to get the model\n","def getRWEModel(\n","    embedding_size_input, embedding_size_output, embedding_weights, hidden_size, dropout\n","):\n","    vocab_size = len(embedding_weights)\n","    model = RWE_Model(\n","        embedding_size_input,\n","        embedding_size_output,\n","        embedding_weights,\n","        hidden_size,\n","        dropout,\n","    )\n","    criterion = torch.nn.MSELoss()\n","    return model.cuda(), criterion"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_v-2rDTf40FH"},"outputs":[],"source":["# @title Helper functions to train the model\n","def load_vocab_embeddings(input_path):\n","    first_line = True\n","    vocab = set()\n","    input_file_relations = open(input_file_relations, \"r\", encoding=\"utf-8\")\n","    for line in input_file_relations:\n","        if first_line == True:\n","            first_line = False\n","        else:\n","            vocab.add(line.strip().split(\" \")[0])\n","    return vocab\n","\n","\n","def load_word_vocab_from_relation_vectors(input_path):\n","    pre_word_vocab = set()\n","    first_line = True\n","    input_file_relations = open(input_path, \"r\", encoding=\"utf-8\")\n","    for line in input_file_relations:\n","        linesplit = line.strip().split(\" \")\n","        if first_line == True:\n","            first_line = False\n","        else:\n","            relation = linesplit[0]\n","            if \"__\" not in relation:\n","                sys.exit(\"ERROR: Pair '\" + relation + \"' does not contain underscore\")\n","            relation_split = relation.rsplit(\"__\", 1)\n","            word1 = relation_split[0]\n","            word2 = relation_split[1]\n","            pre_word_vocab.add(word1)\n","            pre_word_vocab.add(word2)\n","    return pre_word_vocab\n","\n","\n","def load_embeddings_filtered_byvocab(input_path, vocab):\n","    word2index = {}\n","    index2word = {}\n","    matrix_word_embeddings = []\n","    first_line = True\n","    input_file_relations = open(input_path, \"r\", encoding=\"utf-8\")\n","    cont = 0\n","    for line in input_file_relations:\n","        linesplit = line.strip().split(\" \")\n","        if first_line == True:\n","            dimensions = int(linesplit[1])\n","            first_line = False\n","        else:\n","            word = linesplit[0]\n","            if word in vocab and word not in word2index:\n","                word2index[word] = cont\n","                index2word[cont] = word\n","                cont += 1\n","                matrix_word_embeddings.append(\n","                    np.asarray([float(dim) for dim in linesplit[1 : dimensions + 1]])\n","                )\n","    return matrix_word_embeddings, word2index, index2word, dimensions\n","\n","\n","def load_training_data(input_path, matrix_word_embeddings, word2index):\n","    matrix_input = []\n","    matrix_output = []\n","    first_line = True\n","    input_file_relations = open(input_path, \"r\", encoding=\"utf-8\")\n","    for line in input_file_relations:\n","        linesplit = line.strip().split(\" \")\n","        if first_line == True:\n","            dimensions = int(str(line.split(\" \")[1]))\n","            first_line = False\n","        else:\n","            relation = linesplit[0]\n","            if \"__\" not in relation:\n","                sys.exit(\"ERROR: Pair '\" + relation + \"' does not contain underscore\")\n","            relation_split = relation.rsplit(\"__\", 1)\n","            word1 = relation_split[0]\n","            word2 = relation_split[1]\n","            if word1 in word2index and word2 in word2index:\n","                matrix_input.append(np.asarray([word2index[word1], word2index[word2]]))\n","                matrix_output.append(\n","                    np.asarray([float(dim) for dim in linesplit[1 : dimensions + 1]])\n","                )\n","    return matrix_input, matrix_output, dimensions\n","\n","\n","def split_training_data(matrix_input, matrix_output, devsize, batchsize):\n","    matrix_input_train = []\n","    matrix_output_train = []\n","    matrix_input_dev = []\n","    matrix_output_dev = []\n","    num_instances = int((len(matrix_input) // batchsize) * batchsize)\n","    final_size_dev = int(((num_instances * devsize) // batchsize) * batchsize)\n","    final_size_train = int(((num_instances - final_size_dev) // batchsize) * batchsize)\n","    print(\"Size train set: \" + str(final_size_train))\n","    print(\"Size dev set: \" + str(final_size_dev))\n","    all_instances = range(num_instances)\n","    list_index_dev = random.sample(all_instances, final_size_dev)\n","    for i in range(num_instances):\n","        if i in list_index_dev:\n","            matrix_input_dev.append(matrix_input[i])\n","            matrix_output_dev.append(matrix_output[i])\n","        else:\n","            matrix_input_train.append(matrix_input[i])\n","            matrix_output_train.append(matrix_output[i])\n","    return matrix_input_train, matrix_output_train, matrix_input_dev, matrix_output_dev\n","\n","\n","def trainIntervals(model, optimizer, criterion, batches, interval=100, lr=0.1):\n","    i = 0\n","    n = 0\n","    trainErr = 0\n","    for x1, x2, y in zip(*batches):\n","        model.train()\n","        optimizer.zero_grad()\n","        trainErr += gradUpdate(model, x1, x2, y, criterion, optimizer, lr)\n","        i += 1\n","        if i == interval:\n","            n += 1\n","            prev_train_err = trainErr\n","            trainErr = 0\n","            i = 0\n","    if i > 0 and prev_train_err != 0:\n","        print(\"Training error: \" + str(prev_train_err / float(i)))\n","\n","\n","def validate(model, batches, criterion):\n","    evalErr = 0\n","    n = 0\n","    model.eval()\n","    for x1, x2, y in zip(*batches):\n","        y = torch.autograd.Variable(y, requires_grad=False)\n","        x1 = torch.autograd.Variable(x1, requires_grad=False)\n","        x2 = torch.autograd.Variable(x2, requires_grad=False)\n","        output = model(x1, x2)\n","        error = criterion(output, y)\n","        evalErr += error.item()\n","        n += 1\n","    return evalErr / n\n","\n","\n","def gradUpdate(model, x1, x2, y, criterion, optimizer, lr):\n","    output = model(x1, x2)\n","    error = criterion(output, y)\n","    error.backward()\n","    optimizer.step()\n","    return error.item()\n","\n","\n","def getBatches(data, batchSize):\n","    embsize = int(data.size(-1))\n","    return data.view(-1, batchSize, embsize)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"AZgTHAFf33nL"},"outputs":[],"source":["# Define function to train the model\n","def trainEpochs(\n","    model,\n","    optimizer,\n","    criterion,\n","    trainBatches,\n","    validBatches,\n","    epochs=10,\n","    interval=100,\n","    lr=0.1,\n","):\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, patience=2, threshold=1e-7, factor=0.9\n","    )\n","    min_error = -1.0\n","    for epoch in range(1, epochs + 1):\n","        print(\"\\n     ----------    \\n\")\n","        print(\"EPOCH \" + str(epoch))\n","        print(\"Starting training epoch \" + str(epoch))\n","        trainIntervals(model, optimizer, criterion, trainBatches, interval, lr)\n","        validErr = validate(model, validBatches, criterion)\n","        scheduler.step(validErr)\n","        print(\"Validation error : \" + str(validErr))\n","        if validErr < min_error or min_error == -1.0:\n","            new_model = model\n","            min_error = validErr\n","            print(\n","                \"[Model at epoch \"\n","                + str(epoch)\n","                + \" obtained the lowest development error rate so far.]\"\n","            )\n","        # if epoch % 5 == 0 or epoch == 1: torch.save(model, f\"./model-epoch{str(epoch)}.model\")\n","        torch.save(model, f\"epoch-{epoch}.model\")\n","        print(\"Epoch \" + str(epoch) + \" done\")\n","    return new_model"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Wuu2SgMm5uYd"},"outputs":[],"source":["# @title Define driver function to actually load the data for the model training\n","def train_rwe(\n","    word_embeddings_path,\n","    rel_embeddings_path,\n","    output_path,\n","    hidden_size=0,\n","    dropout=0.5,\n","    epochs=5,\n","    interval=100,\n","    batchsize=10,\n","    dev_size=0.015,\n","    lr=0.01,\n","):\n","    if dev_size >= 1 or dev_size < 0:\n","        raise Exception(\n","            \"Development data should be between 0% (0.0) and 100% (1.0) of the training data\"\n","        )\n","\n","    print(\"Loading word vocabulary...\")\n","    pre_word_vocab = load_word_vocab_from_relation_vectors(rel_embeddings_path)\n","    print(\n","        \"Word vocabulary loaded succesfully (\"\n","        + str(len(pre_word_vocab))\n","        + \" words). Now loading word embeddings...\"\n","    )\n","    (\n","        matrix_word_embeddings,\n","        word2index,\n","        index2word,\n","        dims_word,\n","    ) = load_embeddings_filtered_byvocab(word_embeddings_path, pre_word_vocab)\n","    pre_word_vocab.clear()\n","    print(\n","        \"Word embeddings loaded succesfully (\"\n","        + str(dims_word)\n","        + \" dimensions). Now loading relation vectors...\"\n","    )\n","    matrix_input, matrix_output, dims_rels = load_training_data(\n","        rel_embeddings_path, matrix_word_embeddings, word2index\n","    )\n","    print(\n","        \"Relation vectors loaded (\"\n","        + str(dims_rels)\n","        + \" dimensions), now spliting training and dev...\"\n","    )\n","    random.seed(21)\n","    s1 = random.getstate()\n","    random.shuffle(matrix_input)\n","    random.setstate(s1)\n","    random.shuffle(matrix_output)\n","    (\n","        matrix_input_train,\n","        matrix_output_train,\n","        matrix_input_dev,\n","        matrix_output_dev,\n","    ) = split_training_data(matrix_input, matrix_output, dev_size, batchsize)\n","    matrix_input.clear()\n","    matrix_output.clear()\n","    print(\"Done preprocessing all the data, now loading and training the model...\\n\")\n","\n","    if hidden_size == 0:\n","        hidden_size = dims_word * 2\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"Device used: \" + str(device))\n","    embedding_weights = torch.tensor(matrix_word_embeddings)\n","    matrix_word_embeddings.clear()\n","    tensor_input_train_1 = torch.LongTensor([[x[0]] for x in matrix_input_train])\n","    tensor_input_train_2 = torch.LongTensor([[x[1]] for x in matrix_input_train])\n","    matrix_input_train.clear()\n","    tensor_input_dev_1 = torch.LongTensor([[x[0]] for x in matrix_input_dev])\n","    tensor_input_dev_2 = torch.LongTensor([[x[1]] for x in matrix_input_dev])\n","    matrix_input_dev.clear()\n","    tensor_output_train = torch.FloatTensor(matrix_output_train)\n","    matrix_output_train.clear()\n","    tensor_output_dev = torch.FloatTensor(matrix_output_dev)\n","    matrix_output_dev.clear()\n","    model, criterion = getRWEModel(\n","        dims_word, dims_rels, embedding_weights, hidden_size, dropout\n","    )\n","    print(\"RWE model loaded.\")\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","    trainX1batches = getBatches(tensor_input_train_1.cuda(), batchsize)\n","    trainX2batches = getBatches(tensor_input_train_2.cuda(), batchsize)\n","    validX1Batches = getBatches(tensor_input_dev_1.cuda(), batchsize)\n","    validX2Batches = getBatches(tensor_input_dev_2.cuda(), batchsize)\n","    trainYBatches = getBatches(tensor_output_train.cuda(), batchsize)\n","    validYBatches = getBatches(tensor_output_dev.cuda(), batchsize)\n","    print(\"Now starting training...\\n\")\n","    output_model = trainEpochs(\n","        model,\n","        optimizer,\n","        criterion,\n","        (trainX1batches, trainX2batches, trainYBatches),\n","        (validX1Batches, validX2Batches, validYBatches),\n","        epochs,\n","        interval,\n","        lr,\n","    )\n","    print(\n","        \"\\nTraining finished. Now loading relational word embeddings from trained model...\"\n","    )\n","\n","    parameters = list(output_model.parameters())\n","    num_vectors = len(parameters[0])\n","    print(\"Number of vectors: \" + str(num_vectors))\n","    num_dimensions = len(parameters[0][0])\n","    print(\"Number of dimensions output embeddings: \" + str(num_dimensions))\n","    txtfile = open(output_path, \"w\", encoding=\"utf8\")\n","    txtfile.write(str(num_vectors) + \" \" + str(num_dimensions) + \"\\n\")\n","    if num_vectors != embedding_weights.size()[0]:\n","        print(\n","            \"Something is wrong in the input vectors: \"\n","            + str(embedding_weights.size()[0])\n","            + \" != \"\n","            + str(num_vectors)\n","        )\n","    for i in range(num_vectors):\n","        word = index2word[i]\n","        txtfile.write(word)\n","        vector = parameters[0][i].cpu().detach().numpy()\n","        for dimension in vector:\n","            txtfile.write(\" \" + str(dimension))\n","        txtfile.write(\"\\n\")\n","    txtfile.close()\n","    print(\"\\nFINISHED. Word embeddings stored at \" + output_path)"]},{"cell_type":"markdown","metadata":{"id":"WWd4WAxV5MEg"},"source":["## **C. Train Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4237272,"status":"ok","timestamp":1695010274174,"user":{"displayName":"Luthfi Balaka","userId":"17563642752439424669"},"user_tz":-420},"id":"HVp5BkDk5SlT","outputId":"de9f9d79-1e06-4ad6-878a-a98785570156"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading word vocabulary...\n","Word vocabulary loaded succesfully (38896 words). Now loading word embeddings...\n","Word embeddings loaded succesfully (300 dimensions). Now loading relation vectors...\n","Relation vectors loaded (300 dimensions), now spliting training and dev...\n","Size train set: 306910\n","Size dev set: 4670\n","Done preprocessing all the data, now loading and training the model...\n","\n","Device used: cuda\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_908512/2461359559.py:65: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n","  embedding_weights = torch.tensor(matrix_word_embeddings)\n"]},{"name":"stdout","output_type":"stream","text":["RWE model loaded.\n","Now starting training...\n","\n","\n","     ----------    \n","\n","EPOCH 1\n","Starting training epoch 1\n","Training error: 2.0424039929613964\n","Validation error : 3.102219146080076\n","[Model at epoch 1 obtained the lowest development error rate so far.]\n","Epoch 1 done\n","\n","     ----------    \n","\n","EPOCH 2\n","Starting training epoch 2\n","Training error: 2.0074428584795077\n","Validation error : 3.093893662940639\n","[Model at epoch 2 obtained the lowest development error rate so far.]\n","Epoch 2 done\n","\n","     ----------    \n","\n","EPOCH 3\n","Starting training epoch 3\n","Training error: 2.007610808239206\n","Validation error : 3.0983719646819017\n","Epoch 3 done\n","\n","     ----------    \n","\n","EPOCH 4\n","Starting training epoch 4\n","Training error: 2.010543911789472\n","Validation error : 3.098701028404578\n","Epoch 4 done\n","\n","     ----------    \n","\n","EPOCH 5\n","Starting training epoch 5\n","Training error: 2.0085147973764075\n","Validation error : 3.0984100414882763\n","Epoch 5 done\n","\n","     ----------    \n","\n","EPOCH 6\n","Starting training epoch 6\n"]}],"source":["# train the model\n","train_rwe(\n","    \"ft_word_embeddings.txt\",\n","    \"relative_init_vectors.txt\",\n","    \"rwe_embeddings.txt\",\n","    epochs=10,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14S6WiM0lKAi"},"outputs":[],"source":["# from google.colab import files\n","# files.download('/content/rwe_embeddings.txt')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMgbJ7ppYneOiptodPwxstC","collapsed_sections":["VAFTJRdp4LFo"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
